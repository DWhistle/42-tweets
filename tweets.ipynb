{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import namedtuple, defaultdict\n",
    "import math\n",
    "from nltk.corpus import wordnet\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment = namedtuple('Sentiment', [\n",
    "    'type',\n",
    "    'ordinal',\n",
    "])\n",
    "\n",
    "Approach = namedtuple('Approach', [\n",
    "    'binary',\n",
    "    'counts',\n",
    "    'tfidf',\n",
    "    'df_y'\n",
    "])\n",
    "\n",
    "Classifier = namedtuple('Classifier', [\n",
    "    'model',\n",
    "    'params'\n",
    "])\n",
    "\n",
    "approaches = {\n",
    "    'tokenization': None,\n",
    "    'stemming': None,\n",
    "    'lemmatization': None,\n",
    "    's+m': None,\n",
    "    'l+m': None,\n",
    "}\n",
    "\n",
    "sentiments = {\n",
    "    'negative': Sentiment('negative', -1),\n",
    "    'positive': Sentiment('positive', 1),\n",
    "    'neutral': Sentiment('neutral', 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    \n",
    "    def __init__(self, instance):\n",
    "        if instance is None:\n",
    "            self._instance = instance\n",
    "        elif not isinstance(instance, Processor):\n",
    "            raise TypeError(f\"incorrect processor usage {instance}\")\n",
    "        else:\n",
    "            self._instance = instance\n",
    "            self.records = instance.records\n",
    "        \n",
    "    def process_all(self):\n",
    "        if self._instance is None:\n",
    "            return self.process()\n",
    "        else:\n",
    "            self.records = self._instance.process_all()\n",
    "            return self.process()    \n",
    "    \n",
    "    def process(self):\n",
    "        raise NotImplementedError(\"incorrect processor usage\")\n",
    "\n",
    "    \n",
    "class Parser:\n",
    "    non_word_regex = re.compile(r\"[^0-9^A-Z^a-z^ ]\")\n",
    "    @classmethod\n",
    "    def filter_non_words(cls, text):\n",
    "        return Parser.non_word_regex.sub('', text).lower()\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = pd.DataFrame(data={'col': df.items()}, index = range(df.shape[1]))\n",
    "        \n",
    "        return list(df['col'].apply(lambda r: Parser.filter_non_words(r[0])))\n",
    "\n",
    "class Extractor:\n",
    "    ascii_word_regex = re.compile(r\"[0-9A-Za-z]+\")\n",
    "    \n",
    "    @classmethod\n",
    "    def extract_words(cls, text):\n",
    "        return cls.ascii_word_regex.findall(text)\n",
    "        \n",
    "    \n",
    "class Tokenizer(Processor):\n",
    "    \n",
    "    def __init__(self, next_pipeline, sentiment, records=[]):\n",
    "        self.records = records\n",
    "        self.sentiment = sentiments.get(sentiment)\n",
    "        super().__init__(next_pipeline)\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        words = Extractor.extract_words(text)\n",
    "        wc = Counter(words)\n",
    "        wc['tweet'] = text\n",
    "        return dict(wc)\n",
    "\n",
    "    def format_to_row(self, wc):\n",
    "        wc['sentiment'] = self.sentiment.ordinal\n",
    "        return wc\n",
    "\n",
    "    def process(self):\n",
    "        wc = (self.count_tokens(text) for text in self.records)\n",
    "        final_rows = [self.format_to_row(wcount) for wcount in wc]\n",
    "        return final_rows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def rearrange(self):\n",
    "        df = self.df\n",
    "        cols = list(df.columns)\n",
    "        cols.remove('sentiment')\n",
    "        cols.remove('tweet')\n",
    "        self.df = df[['sentiment', 'tweet'] + cols]\n",
    "        \n",
    "    def split(self):\n",
    "        df = self.df\n",
    "        df.fillna(0, inplace=True)\n",
    "        self.df_x = df.iloc[:, 2:].astype(int)\n",
    "        self.df_y = df.iloc[:, :2]\n",
    "        \n",
    "    def clean(self):\n",
    "        df = self.df\n",
    "        self.df.drop_duplicates(subset='tweet', inplace=True, keep='last')\n",
    "        cols = filter(lambda c: not c.isnumeric(), df.columns)\n",
    "        self.df = df[cols]\n",
    "        \n",
    "    def execute(self):\n",
    "        self.clean()\n",
    "        self.rearrange()\n",
    "        self.split()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"foo\">\n",
    "\n",
    "| approach | 0 or 1, if the word exists | word counts | TFIDF |\n",
    "| --- | --- | --- | --- |\n",
    "| Just tokenization |  | |  |\n",
    "| Stemming |  | |  |\n",
    "| Lemmatization |  | |  |\n",
    "| Stemming + Misspellings |  | |  |\n",
    "| Lemmatization + Misspellings |  | |  |\n",
    "| Any other ... |  | |  |\n",
    "\n",
    " \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(processed_rows):\n",
    "    return processed_rows.apply(lambda r: [v & 1 for v in r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFProcessor:\n",
    "    def __init__(self, rows: pd.DataFrame):\n",
    "        self.rows = rows.copy()\n",
    "        self.num_of_texts = rows.shape[0]\n",
    "        self.num_of_apparitions = dict(TFIDFProcessor.binarize(rows).sum())\n",
    "    \n",
    "    @staticmethod\n",
    "    def binarize(rows):\n",
    "        return rows.apply(lambda r: [v & 1 for v in r])\n",
    "\n",
    "    def compute_tf(self):\n",
    "        return self.rows.apply(lambda r: r / sum(r), axis = 1)\n",
    "    \n",
    "    def compute_idf(self):\n",
    "        term_importances = {}\n",
    "        for w, c in self.num_of_apparitions.items():\n",
    "            num_of_occs = 1.0 if c <= 0 else float(c)\n",
    "            term_importances[w] = math.log10(float(self.num_of_texts) / num_of_occs)\n",
    "        return term_importances\n",
    "    \n",
    "    def compute_tfidf(self):\n",
    "        tf_dataset = self.compute_tf()\n",
    "        for word, importance in self.compute_idf().items():\n",
    "            tf_dataset[word] *= importance * 100000\n",
    "        tf_dataset.fillna(0, inplace=True)\n",
    "        return tf_dataset.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets = Parser.parse('./data/processedNegative.csv')\n",
    "positive_tweets = Parser.parse('./data/processedPositive.csv')\n",
    "neutral_tweets = Parser.parse('./data/processedNeutral.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = Tokenizer(None, 'negative', records=negative_tweets).process_all() \\\n",
    "            + Tokenizer(None, 'positive', records=positive_tweets).process_all() \\\n",
    "            + Tokenizer(None, 'neutral', records=neutral_tweets).process_all()\n",
    "df_token = pd.DataFrame(all_tweets)\n",
    "\n",
    "preprocessor = PreProcessor(df_token)\n",
    "preprocessor.execute()\n",
    "\n",
    "tfidf = TFIDFProcessor(preprocessor.df_x)\n",
    "approaches['tokenization'] = Approach(binarize(preprocessor.df_x), preprocessor.df_x, tfidf.compute_tfidf(), preprocessor.df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stemmer(Processor):\n",
    "    \n",
    "    def __init__(self, next_pipeline, records = []):\n",
    "        self.records = records\n",
    "        self.stemmer = PorterStemmer()\n",
    "        super().__init__(next_pipeline)\n",
    "            \n",
    "    def process(self):\n",
    "        texts = []\n",
    "        for text in self.records:\n",
    "            words = []\n",
    "            for w in Extractor.extract_words(text):\n",
    "                words.append(self.stemmer.stem(w))\n",
    "            texts.append(' '.join(words))\n",
    "        return texts\n",
    "    \n",
    "all_stemmed = Tokenizer(Stemmer(None, records = negative_tweets), 'negative').process_all() \\\n",
    "+ Tokenizer(Stemmer(None, records=positive_tweets), 'positive').process_all() \\\n",
    "+ Tokenizer(Stemmer(None, records=neutral_tweets), 'neutral').process_all() \n",
    "\n",
    "df_stemmed = pd.DataFrame(all_stemmed)\n",
    "pp_stemmed = PreProcessor(df_stemmed)\n",
    "pp_stemmed.execute()\n",
    "\n",
    "tfidf = TFIDFProcessor(pp_stemmed.df_x)\n",
    "approaches['stemming'] = Approach(binarize(pp_stemmed.df_x), pp_stemmed.df_x, tfidf.compute_tfidf(), pp_stemmed.df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>how</th>\n",
       "      <th>unhappi</th>\n",
       "      <th>some</th>\n",
       "      <th>dog</th>\n",
       "      <th>like</th>\n",
       "      <th>it</th>\n",
       "      <th>though</th>\n",
       "      <th>talk</th>\n",
       "      <th>to</th>\n",
       "      <th>my</th>\n",
       "      <th>...</th>\n",
       "      <th>idfc</th>\n",
       "      <th>vikram</th>\n",
       "      <th>limay</th>\n",
       "      <th>diana</th>\n",
       "      <th>edulji</th>\n",
       "      <th>cag</th>\n",
       "      <th>4member</th>\n",
       "      <th>amulya</th>\n",
       "      <th>agmut</th>\n",
       "      <th>cadr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3821 rows × 5339 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      how  unhappi  some  dog  like  it  though  talk  to  my  ...  idfc  \\\n",
       "0       1        1     1    1     1   1       1     0   0   0  ...     0   \n",
       "1       0        0     0    0     0   1       0     1   3   1  ...     0   \n",
       "2       0        1     1    0     1   1       0     0   2   0  ...     0   \n",
       "3       0        1     0    0     0   0       0     0   1   0  ...     0   \n",
       "4       0        1     0    0     0   0       0     0   0   0  ...     0   \n",
       "...   ...      ...   ...  ...   ...  ..     ...   ...  ..  ..  ...   ...   \n",
       "3868    0        0     0    0     0   0       0     0   0   0  ...     1   \n",
       "3869    0        0     0    0     0   0       0     0   1   0  ...     0   \n",
       "3870    0        0     0    0     0   0       0     0   1   0  ...     0   \n",
       "3871    0        0     0    0     0   0       0     0   0   0  ...     0   \n",
       "3872    0        0     0    0     0   0       0     0   0   0  ...     0   \n",
       "\n",
       "      vikram  limay  diana  edulji  cag  4member  amulya  agmut  cadr  \n",
       "0          0      0      0       0    0        0       0      0     0  \n",
       "1          0      0      0       0    0        0       0      0     0  \n",
       "2          0      0      0       0    0        0       0      0     0  \n",
       "3          0      0      0       0    0        0       0      0     0  \n",
       "4          0      0      0       0    0        0       0      0     0  \n",
       "...      ...    ...    ...     ...  ...      ...     ...    ...   ...  \n",
       "3868       1      1      0       0    0        0       0      0     0  \n",
       "3869       0      0      1       1    0        0       0      0     0  \n",
       "3870       0      0      0       0    1        1       0      0     0  \n",
       "3871       0      0      0       0    0        0       0      0     0  \n",
       "3872       0      0      0       0    0        0       1      1     1  \n",
       "\n",
       "[3821 rows x 5339 columns]"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approaches['stemming'].counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(Processor):\n",
    "    def __init__(self, next_pileline, records=[]):\n",
    "        self.records = records\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        super().__init__(next_pileline)\n",
    "\n",
    "        \n",
    "    def process(self):\n",
    "        texts = []\n",
    "        for text in self.records:\n",
    "            words = []\n",
    "            for w in Extractor.extract_words(text):\n",
    "                words.append(self.lemmatizer.lemmatize(w, pos=Lemmatizer.get_wordnet_pos(w)))\n",
    "            texts.append(' '.join(words))\n",
    "        return texts\n",
    "\n",
    "    @classmethod\n",
    "    def get_wordnet_pos(cls, word):\n",
    "        pos = nltk.pos_tag([word])\n",
    "        treebank_tag = pos[0][1]\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "\n",
    "all_lemmed = Tokenizer(Lemmatizer(None, records = negative_tweets), 'negative').process_all() \\\n",
    "+ Tokenizer(Lemmatizer(None, records = positive_tweets), 'positive').process_all() \\\n",
    "+ Tokenizer(Lemmatizer(None, records = neutral_tweets), 'neutral').process_all() \n",
    "\n",
    "pp_lemmed = PreProcessor(pd.DataFrame(all_lemmed))\n",
    "pp_lemmed.execute()\n",
    "\n",
    "tfidf_lemmed = TFIDFProcessor(pp_lemmed.df_x)\n",
    "\n",
    "approaches['lemmatization'] = Approach(binarize(pp_lemmed.df_x), pp_lemmed.df_x, tfidf_lemmed.compute_tfidf(), pp_lemmed.df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misspelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MisspellingsCorrector(Processor):\n",
    "    \n",
    "    def init_symspell(self):\n",
    "        sym_spell = SymSpell(max_dictionary_edit_distance=1)\n",
    "        dictionary_path = pkg_resources.resource_filename(\n",
    "            \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "        sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "        self.sym_spell = sym_spell\n",
    "        \n",
    "    def __init__(self, next_pipeline, records=[]):\n",
    "        self.records = records\n",
    "        self.init_symspell()\n",
    "        super().__init__(next_pipeline)\n",
    "    \n",
    "    def process(self):\n",
    "        texts = []\n",
    "        for text in self.records:\n",
    "            words = []\n",
    "            for w in Extractor.extract_words(text):\n",
    "                words.append(self.correct_word(w))\n",
    "            texts.append(' '.join(words))\n",
    "        return texts\n",
    "    \n",
    "    def correct_word(self, word: str) -> str:\n",
    "        corrections = self.sym_spell.lookup(word, Verbosity.CLOSEST, ignore_token=r\"\\w+\\d\")\n",
    "        if len(corrections) == 0:\n",
    "            return ''\n",
    "        else:\n",
    "            return corrections[0].term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming + misspellings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stemmed_corrected = \\\n",
    "Tokenizer(Stemmer(MisspellingsCorrector(None, records=negative_tweets)), 'negative').process_all() \\\n",
    "+ Tokenizer(Stemmer(MisspellingsCorrector(None, records=positive_tweets)), 'positive').process_all() \\\n",
    "+ Tokenizer(Stemmer(MisspellingsCorrector(None, records=neutral_tweets)), 'neutral').process_all() \n",
    "\n",
    "pp_stemmed_corrected = PreProcessor(pd.DataFrame(all_stemmed_corrected))\n",
    "pp_stemmed_corrected.execute()\n",
    "\n",
    "tfidf_stemmed_corrected = TFIDFProcessor(pp_stemmed_corrected.df_x)\n",
    "\n",
    "approaches['s+m'] = Approach(binarize(pp_stemmed_corrected.df_x), pp_stemmed_corrected.df_x, tfidf_stemmed_corrected.compute_tfidf(), pp_stemmed_corrected.df_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + misspellings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemmed_corrected = Tokenizer(Lemmatizer(MisspellingsCorrector(None, records=negative_tweets)), 'negative').process_all() \\\n",
    "+ Tokenizer(Lemmatizer(MisspellingsCorrector(None, records=positive_tweets)), 'positive').process_all() \\\n",
    "+ Tokenizer(Lemmatizer(MisspellingsCorrector(None, records=neutral_tweets)), 'neutral').process_all() \n",
    "\n",
    "pp_lemmed_corrected = PreProcessor(pd.DataFrame(all_lemmed_corrected))\n",
    "pp_lemmed_corrected.execute()\n",
    "\n",
    "tfidf_lemmed_corrected = TFIDFProcessor(pp_lemmed_corrected.df_x)\n",
    "\n",
    "approaches['l+m'] = Approach(binarize(pp_lemmed_corrected.df_x), pp_lemmed_corrected.df_x, tfidf_lemmed_corrected.compute_tfidf(), pp_lemmed_corrected.df_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>how</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>some</th>\n",
       "      <th>dogs</th>\n",
       "      <th>like</th>\n",
       "      <th>it</th>\n",
       "      <th>though</th>\n",
       "      <th>talking</th>\n",
       "      <th>to</th>\n",
       "      <th>my</th>\n",
       "      <th>...</th>\n",
       "      <th>vikram</th>\n",
       "      <th>limaye</th>\n",
       "      <th>diana</th>\n",
       "      <th>edulji</th>\n",
       "      <th>cag</th>\n",
       "      <th>4member</th>\n",
       "      <th>amulya</th>\n",
       "      <th>appointed</th>\n",
       "      <th>agmut</th>\n",
       "      <th>cadre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22113</td>\n",
       "      <td>10040</td>\n",
       "      <td>27601</td>\n",
       "      <td>40102</td>\n",
       "      <td>23440</td>\n",
       "      <td>19001</td>\n",
       "      <td>34845</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10038</td>\n",
       "      <td>8929</td>\n",
       "      <td>5263</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2510</td>\n",
       "      <td>6900</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5102</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>8785</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8929</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>10040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>89633</td>\n",
       "      <td>89633</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7143</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35853</td>\n",
       "      <td>35853</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5102</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25609</td>\n",
       "      <td>25609</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22408</td>\n",
       "      <td>22408</td>\n",
       "      <td>22408</td>\n",
       "      <td>22408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3849 rows × 6609 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        how  unhappy   some   dogs   like     it  though  talking    to    my  \\\n",
       "0     22113    10040  27601  40102  23440  19001   34845        0     0     0   \n",
       "1         0        0      0      0      0      0       0    10038  8929  5263   \n",
       "2         0     2510   6900      0      0   4750       0        0  5102     0   \n",
       "3         0     8785      0      0      0      0       0        0  8929     0   \n",
       "4         0    10040      0      0      0      0       0        0     0     0   \n",
       "...     ...      ...    ...    ...    ...    ...     ...      ...   ...   ...   \n",
       "3868      0        0      0      0      0      0       0        0     0     0   \n",
       "3869      0        0      0      0      0      0       0        0  7143     0   \n",
       "3870      0        0      0      0      0      0       0        0  5102     0   \n",
       "3871      0        0      0      0      0      0       0        0     0     0   \n",
       "3872      0        0      0      0      0      0       0        0     0     0   \n",
       "\n",
       "      ...  vikram  limaye  diana  edulji    cag  4member  amulya  appointed  \\\n",
       "0     ...       0       0      0       0      0        0       0          0   \n",
       "1     ...       0       0      0       0      0        0       0          0   \n",
       "2     ...       0       0      0       0      0        0       0          0   \n",
       "3     ...       0       0      0       0      0        0       0          0   \n",
       "4     ...       0       0      0       0      0        0       0          0   \n",
       "...   ...     ...     ...    ...     ...    ...      ...     ...        ...   \n",
       "3868  ...   89633   89633      0       0      0        0       0          0   \n",
       "3869  ...       0       0  35853   35853      0        0       0          0   \n",
       "3870  ...       0       0      0       0  25609    25609       0          0   \n",
       "3871  ...       0       0      0       0      0        0       0          0   \n",
       "3872  ...       0       0      0       0      0        0   22408      22408   \n",
       "\n",
       "      agmut  cadre  \n",
       "0         0      0  \n",
       "1         0      0  \n",
       "2         0      0  \n",
       "3         0      0  \n",
       "4         0      0  \n",
       "...     ...    ...  \n",
       "3868      0      0  \n",
       "3869      0      0  \n",
       "3870      0      0  \n",
       "3871      0      0  \n",
       "3872  22408  22408  \n",
       "\n",
       "[3849 rows x 6609 columns]"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approaches['tokenization'].tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000, solver='newton-cg')"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(approaches['s+m'].counts, approaches['s+m'].df_y.sentiment, test_size=0.5)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, solver='newton-cg')\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8804523424878837"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(lr.predict(X_test), y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[448,   6,  13],\n",
       "       [ 51, 732,  89],\n",
       "       [ 35,  28, 455]])"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(lr.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7996768982229402"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(7)\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy_score(knn.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"logistic\": Classifier(LogisticRegression, {\"C\": [1.0, 2.0, 0.5, 0.25], \"solver\": ('newton-cg', 'sag', 'saga'), \"max_iter\": [500]}),\n",
    "    \"randomforest\": Classifier(RandomForestClassifier, dict(n_estimators = [100, 300, 500], max_depth = [ 25, 30], min_samples_split = [2, 5], min_samples_leaf = [1, 2])),\n",
    "    \"knn\": Classifier(KNeighborsClassifier, dict(n_neighbors=range(2,7), algorithm=['ball_tree', 'kd_tree', 'auto'])),\n",
    "    \"decisiontree\": Classifier(DecisionTreeClassifier, dict(max_features=['sqrt', 'log2', None], criterion=[\"gini\", \"entropy\"], min_samples_split=[2,3,4]))\n",
    "    }\n",
    "\n",
    "def optimize_model_params(classifier: Classifier, x_train, y_train):\n",
    "    gs = GridSearchCV(classifier.model(), param_grid=classifier.params, n_jobs=-1)\n",
    "    gs.fit(x_train, y_train)\n",
    "    return gs.best_params_, gs.best_score_\n",
    "\n",
    "def find_best_model(df_x, df_y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.3)\n",
    "    max_accuracy = 0\n",
    "    best_model = None\n",
    "    for name, model in models.items():\n",
    "        print(f'optimizing {name}')\n",
    "        best_params, best_accuracy = optimize_model_params(model, X_train, y_train)\n",
    "        print(f'Best accuracy {best_accuracy} for model: {name}')\n",
    "        if best_accuracy > max_accuracy:\n",
    "            max_accuracy = best_accuracy\n",
    "            best_model = Classifier(model.model, best_params)\n",
    "    return best_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approach tokenization\n",
      "optimizing logistic\n",
      "Best accuracy 0.8934651116276182 for model: logistic\n",
      "optimizing randomforest\n",
      "Best accuracy 0.8808401900807636 for model: randomforest\n",
      "optimizing knn\n",
      "Best accuracy 0.824792573332138 for model: knn\n",
      "optimizing decisiontree\n",
      "Best accuracy 0.8886351566648966 for model: decisiontree\n",
      "\n",
      "Approach stemming\n",
      "optimizing logistic\n",
      "Best accuracy 0.8889355595225595 for model: logistic\n",
      "optimizing randomforest\n",
      "Best accuracy 0.8750946830480592 for model: randomforest\n",
      "optimizing knn\n",
      "Best accuracy 0.8010556897336274 for model: knn\n",
      "optimizing decisiontree\n",
      "Best accuracy 0.8732241240505443 for model: decisiontree\n",
      "\n",
      "Approach lemmatization\n",
      "optimizing logistic\n",
      "Best accuracy 0.9020126710770416 for model: logistic\n",
      "optimizing randomforest\n",
      "Best accuracy 0.881074591340264 for model: randomforest\n",
      "optimizing knn\n",
      "Best accuracy 0.8174944870313976 for model: knn\n",
      "optimizing decisiontree\n",
      "Best accuracy 0.8840631453673563 for model: decisiontree\n",
      "\n",
      "Approach s+m\n",
      "optimizing logistic\n",
      "Best accuracy 0.878411886764488 for model: logistic\n",
      "optimizing randomforest\n",
      "Best accuracy 0.8587913146583667 for model: randomforest\n",
      "optimizing knn\n",
      "Best accuracy 0.802240254928116 for model: knn\n",
      "optimizing decisiontree\n",
      "Best accuracy 0.8564828812805692 for model: decisiontree\n",
      "\n",
      "Approach l+m\n",
      "optimizing logistic\n",
      "Best accuracy 0.8761078998073218 for model: logistic\n",
      "optimizing randomforest\n",
      "Best accuracy 0.863031717800504 for model: randomforest\n",
      "optimizing knn\n",
      "Best accuracy 0.8037831628872091 for model: knn\n",
      "optimizing decisiontree\n",
      "Best accuracy 0.8541759300429821 for model: decisiontree\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained_models = {}\n",
    "for name, approach in approaches.items():\n",
    "    print(f'Approach {name}')\n",
    "    trained_models[name] = find_best_model(approach.counts, approach.df_y.sentiment)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenization': Classifier(model=<class 'sklearn.linear_model._logistic.LogisticRegression'>, params={'C': 2.0, 'max_iter': 500, 'solver': 'sag'}),\n",
       " 'stemming': Classifier(model=<class 'sklearn.linear_model._logistic.LogisticRegression'>, params={'C': 0.5, 'max_iter': 500, 'solver': 'newton-cg'}),\n",
       " 'lemmatization': Classifier(model=<class 'sklearn.linear_model._logistic.LogisticRegression'>, params={'C': 2.0, 'max_iter': 500, 'solver': 'saga'}),\n",
       " 's+m': Classifier(model=<class 'sklearn.linear_model._logistic.LogisticRegression'>, params={'C': 1.0, 'max_iter': 500, 'solver': 'newton-cg'}),\n",
       " 'l+m': Classifier(model=<class 'sklearn.linear_model._logistic.LogisticRegression'>, params={'C': 1.0, 'max_iter': 500, 'solver': 'newton-cg'})}"
      ]
     },
     "execution_count": 1023,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding similar tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from multiprocessing import Pool\n",
    "def compute_cosine_similarity(v1, v2):\n",
    "    return cosine_similarity(v1.values.reshape(1,-1), v2.values.reshape(1,-1))\n",
    "\n",
    "def find_most_similar(distances: dict) -> list:\n",
    "    max_index = 0\n",
    "    max_val = 0.0\n",
    "    for i, si in distances.items():\n",
    "        mapped_si = map(lambda x: x[1], si)\n",
    "        over_sum = sum(sorted(mapped_si)[-11:-1])\n",
    "        if over_sum > max_val:\n",
    "            max_val = over_sum\n",
    "            max_index = i\n",
    "    return sorted(distances[max_index], key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "def find_top10(name, approach: Approach) -> list: \n",
    "    distances = defaultdict(list)\n",
    "    output = []\n",
    "    for i, v in approach.tfidf.iterrows():\n",
    "        for i2, v2 in approach.tfidf.loc[i+1:].iterrows():\n",
    "            cosine_sim = compute_cosine_similarity(v,v2)\n",
    "            distances[i].append((i2, cosine_sim[0][0]))\n",
    "        if i % 500 == 0:\n",
    "            print(f'{name}: {i} procesed')\n",
    "    similar_tweets = find_most_similar(distances)\n",
    "    for si, similarity in similar_tweets:\n",
    "        output.append(approach.df_y['tweet'].loc[si])\n",
    "    return output\n",
    "\n",
    "def process_top10s(approaches):\n",
    "    output = []\n",
    "    for name, approach in approaches.items():\n",
    "        output.append((name, find_top10(name, approach)))\n",
    "    return output\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenization: 0 procesed\n",
      "tokenization: 500 procesed\n",
      "tokenization: 1000 procesed\n",
      "tokenization: 1500 procesed\n",
      "tokenization: 2000 procesed\n",
      "tokenization: 2500 procesed\n",
      "tokenization: 3000 procesed\n",
      "tokenization: 3500 procesed\n",
      "stemming: 0 procesed\n",
      "stemming: 500 procesed\n",
      "stemming: 1000 procesed\n",
      "stemming: 1500 procesed\n",
      "stemming: 2000 procesed\n",
      "stemming: 2500 procesed\n",
      "stemming: 3000 procesed\n",
      "stemming: 3500 procesed\n",
      "lemmatization: 0 procesed\n",
      "lemmatization: 500 procesed\n",
      "lemmatization: 1000 procesed\n",
      "lemmatization: 1500 procesed\n",
      "lemmatization: 2000 procesed\n",
      "lemmatization: 2500 procesed\n",
      "lemmatization: 3000 procesed\n",
      "lemmatization: 3500 procesed\n",
      "s+m: 0 procesed\n",
      "s+m: 500 procesed\n",
      "s+m: 1000 procesed\n",
      "s+m: 1500 procesed\n",
      "s+m: 2000 procesed\n",
      "s+m: 2500 procesed\n",
      "s+m: 3000 procesed\n",
      "s+m: 3500 procesed\n",
      "l+m: 0 procesed\n",
      "l+m: 500 procesed\n",
      "l+m: 1000 procesed\n",
      "l+m: 1500 procesed\n",
      "l+m: 2000 procesed\n",
      "l+m: 2500 procesed\n",
      "l+m: 3000 procesed\n",
      "l+m: 3500 procesed\n"
     ]
    }
   ],
   "source": [
    "results = process_top10s(approaches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: tokenization\n",
      "koalas are dying of thirst  and its all because of us unhappy  1\n",
      "koalas are dying of thirst  and its all because of us unhappy  2\n",
      "koalas are dying of thirst  and its all because of us unhappy  3\n",
      "koalas are dying of thirst  and its all because of us unhappy  4\n",
      "koalas are dying of thirst  and its all because of us unhappy  5\n",
      "koalas are dying of thirst  and its all because of us unhappy  6\n",
      "koalas are dying of thirst  and its all because of us unhappy  7\n",
      "koalas are dying of thirst  and its all because of us unhappy  8\n",
      "koalas are dying of thirst  and its all because of us unhappy  9\n",
      "koalas are dying of thirst  and its all because of us unhappy  10\n",
      "\n",
      "Name: stemming\n",
      "definit my arm unhappi 1\n",
      "definit my arm unhappi 2\n",
      "definit my arm unhappi 3\n",
      "definit my arm unhappi 4\n",
      "definit my arm unhappi 5\n",
      "definit my arm unhappi 6\n",
      "definit my arm unhappi 7\n",
      "definit my arm unhappi 8\n",
      "definit my arm unhappi 9\n",
      "definit my arm unhappi 10\n",
      "\n",
      "Name: lemmatization\n",
      "definitely my arm unhappy 1\n",
      "definitely my arm unhappy 2\n",
      "definitely my arm unhappy 3\n",
      "definitely my arm unhappy 4\n",
      "definitely my arm unhappy 5\n",
      "definitely my arm unhappy 6\n",
      "definitely my arm unhappy 7\n",
      "definitely my arm unhappy 8\n",
      "definitely my arm unhappy 9\n",
      "definitely my arm unhappy 10\n",
      "\n",
      "Name: s+m\n",
      "and more also in paper 11\n",
      "and more also in paper 12\n",
      "and more also in paper 13\n",
      "and more also in paper 14\n",
      "and more also in paper 15\n",
      "and more also in paper 16\n",
      "and more also in paper 17\n",
      "and more also in paper 18\n",
      "and more also in paper 19\n",
      "and more also in paper 20\n",
      "\n",
      "Name: l+m\n",
      "and more also in paper 11\n",
      "and more also in paper 12\n",
      "and more also in paper 13\n",
      "and more also in paper 14\n",
      "and more also in paper 15\n",
      "and more also in paper 16\n",
      "and more also in paper 17\n",
      "and more also in paper 18\n",
      "and more also in paper 19\n",
      "and more also in paper 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name,r in results:\n",
    "    print(f'Name: {name}')\n",
    "    [print(re) for re in r]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8962510897994769"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=2.0, solver='saga', max_iter=500)\n",
    "X_train, X_test, y_train, y_test = train_test_split(approaches['lemmatization'].counts, \n",
    "                                                    approaches['lemmatization'].df_y.sentiment, test_size=0.3)\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
