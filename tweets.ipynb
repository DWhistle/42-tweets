{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "from collections import namedtuple, defaultdict\n",
    "import math\n",
    "from nltk.corpus import wordnet\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sentiment = namedtuple('Sentiment', [\n",
    "    'type',\n",
    "    'ordinal',\n",
    "])\n",
    "\n",
    "Approach = namedtuple('Approach', [\n",
    "    'binary',\n",
    "    'counts',\n",
    "    'tfidf',\n",
    "    'df_y'\n",
    "])\n",
    "\n",
    "approaches = {\n",
    "    'tokenization': None,\n",
    "    'stemming': None,\n",
    "    'lemmatization': None,\n",
    "    's+m': None,\n",
    "    'l+m': None,\n",
    "}\n",
    "\n",
    "sentiments = {\n",
    "    'negative': Sentiment('negative', -1),\n",
    "    'positive': Sentiment('positive', 1),\n",
    "    'neutral': Sentiment('neutral', 0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Processor:\n",
    "    \n",
    "    def __init__(self, instance):\n",
    "        if instance is None:\n",
    "            self._instance = instance\n",
    "        elif not isinstance(instance, Processor):\n",
    "            raise NotImplementedError(f\"incorrect decorator usage {instance}\")\n",
    "        else:\n",
    "            self._instance = instance\n",
    "            self.records = instance.records\n",
    "        \n",
    "    def begin_processing(self):\n",
    "        if self._instance is None:\n",
    "            return self.process()\n",
    "        else:\n",
    "            self.records = self._instance.begin_processing()\n",
    "            return self.process()    \n",
    "    \n",
    "    def process(self):\n",
    "        raise NotImplementedError(\"incorrect decorator usage\")\n",
    "\n",
    "    \n",
    "class Parser:\n",
    "    non_word_regex = re.compile(r\"[^0-9^A-Z^a-z^ ]\")\n",
    "    @classmethod\n",
    "    def filter_non_words(cls, text):\n",
    "        return Parser.non_word_regex.sub('', text).lower()\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse(path):\n",
    "        df = pd.read_csv(path)\n",
    "        df = pd.DataFrame(data={'col': df.items()}, index = range(df.shape[1]))\n",
    "        \n",
    "        return list(df['col'].apply(lambda r: Parser.filter_non_words(r[0])))\n",
    "\n",
    "class Extractor:\n",
    "    ascii_word_regex = re.compile(r\"[0-9A-Za-z]+\")\n",
    "    \n",
    "    @classmethod\n",
    "    def extract_words(cls, text):\n",
    "        return cls.ascii_word_regex.findall(text)\n",
    "        \n",
    "    \n",
    "class Tokenizer(Processor):\n",
    "    \n",
    "    def __init__(self, next_pipeline, sentiment, records=[]):\n",
    "        self.records = records\n",
    "        self.sentiment = sentiments.get(sentiment)\n",
    "        super().__init__(next_pipeline)\n",
    "\n",
    "    def count_tokens(self, text):\n",
    "        words = Extractor.extract_words(text)\n",
    "        wc = Counter(words)\n",
    "        wc['tweet'] = text\n",
    "        return dict(wc)\n",
    "\n",
    "    def format_to_row(self, wc):\n",
    "        wc['sentiment'] = self.sentiment.ordinal\n",
    "        return wc\n",
    "\n",
    "    def process(self):\n",
    "        wc = (self.count_tokens(text) for text in self.records)\n",
    "        final_rows = [self.format_to_row(wcount) for wcount in wc]\n",
    "        return final_rows\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    \n",
    "    def rearrange(self):\n",
    "        df = self.df\n",
    "        cols = list(df.columns)\n",
    "        cols.remove('sentiment')\n",
    "        cols.remove('tweet')\n",
    "        self.df = df[['sentiment', 'tweet'] + cols]\n",
    "        \n",
    "    def split(self):\n",
    "        df = self.df\n",
    "        df.fillna(0, inplace=True)\n",
    "        self.df_x = df.iloc[:, 2:].astype(int)\n",
    "        self.df_y = df.iloc[:, :2]\n",
    "        \n",
    "    def clean(self):\n",
    "        df = self.df\n",
    "        self.df.drop_duplicates(subset='tweet', inplace=True, keep='last')\n",
    "        cols = filter(lambda c: not c.isnumeric(), df.columns)\n",
    "        self.df = df[cols]\n",
    "        \n",
    "    def execute(self):\n",
    "        self.clean()\n",
    "        self.rearrange()\n",
    "        self.split()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"foo\">\n",
    "\n",
    "| approach | 0 or 1, if the word exists | word counts | TFIDF |\n",
    "| --- | --- | --- | --- |\n",
    "| Just tokenization |  | |  |\n",
    "| Stemming |  | |  |\n",
    "| Lemmatization |  | |  |\n",
    "| Stemming + Misspellings |  | |  |\n",
    "| Lemmatization + Misspellings |  | |  |\n",
    "| Any other ... |  | |  |\n",
    "\n",
    " \n",
    "</div>\n",
    "str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize(processed_rows):\n",
    "    return processed_rows.apply(lambda r: [v & 1 for v in r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFProcessor:\n",
    "    def __init__(self, rows: pd.DataFrame):\n",
    "        self.rows = rows.copy()\n",
    "        self.num_of_texts = rows.shape[0]\n",
    "        self.num_of_apparitions = dict(TFIDFProcessor.binarize(rows).sum())\n",
    "    \n",
    "    @staticmethod\n",
    "    def binarize(rows):\n",
    "        return rows.apply(lambda r: [v & 1 for v in r])\n",
    "\n",
    "    def compute_tf(self):\n",
    "        return self.rows.apply(lambda r: r / sum(r), axis = 1)\n",
    "    \n",
    "    def compute_idf(self):\n",
    "        term_importances = {}\n",
    "        for w, c in self.num_of_apparitions.items():\n",
    "            num_of_occs = 1.0 if c <= 0 else float(c)\n",
    "            term_importances[w] = math.log10(float(self.num_of_texts) / num_of_occs)\n",
    "        return term_importances\n",
    "    \n",
    "    def compute_tfidf(self):\n",
    "        tf_dataset = self.compute_tf()\n",
    "        for word, importance in self.compute_idf().items():\n",
    "            tf_dataset[word] *= importance * 1000\n",
    "        tf_dataset.fillna(0, inplace=True)\n",
    "        return tf_dataset.astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_tweets = Parser.parse('./data/processedNegative.csv')\n",
    "positive_tweets = Parser.parse('./data/processedPositive.csv')\n",
    "neutral_tweets = Parser.parse('./data/processedNeutral.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just tokenization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = Tokenizer(None, 'negative', records=negative_tweets).begin_processing() \\\n",
    "            + Tokenizer(None, 'positive', records=positive_tweets).begin_processing() \\\n",
    "            + Tokenizer(None, 'neutral', records=neutral_tweets).begin_processing()\n",
    "df_token = pd.DataFrame(all_tweets)\n",
    "\n",
    "preprocessor = PreProcessor(df_token)\n",
    "preprocessor.execute()\n",
    "\n",
    "tfidf = TFIDFProcessor(preprocessor.df_x)\n",
    "approaches['tokenization'] = Approach(binarize(preprocessor.df_x), preprocessor.df_x, tfidf.compute_tfidf(), preprocessor.df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Stemmer(Processor):\n",
    "    \n",
    "    def __init__(self, next_pipeline, records = []):\n",
    "        self.records = records\n",
    "        self.stemmer = PorterStemmer()\n",
    "        super().__init__(next_pipeline)\n",
    "            \n",
    "    def process(self):\n",
    "        texts = []\n",
    "        for text in self.records:\n",
    "            words = []\n",
    "            for w in Extractor.extract_words(text):\n",
    "                words.append(self.stemmer.stem(w))\n",
    "            texts.append(' '.join(words))\n",
    "        return texts\n",
    "    \n",
    "all_stemmed = Tokenizer(Stemmer(None, records = negative_tweets), 'negative').begin_processing() \\\n",
    "+ Tokenizer(Stemmer(None, records=positive_tweets), 'positive').begin_processing() \\\n",
    "+ Tokenizer(Stemmer(None, records=neutral_tweets), 'neutral').begin_processing() \n",
    "\n",
    "df_stemmed = pd.DataFrame(all_stemmed)\n",
    "pp_stemmed = PreProcessor(df_stemmed)\n",
    "pp_stemmed.execute()\n",
    "\n",
    "tfidf = TFIDFProcessor(pp_stemmed.df_x)\n",
    "approaches['stemming'] = Approach(binarize(pp_stemmed.df_x), pp_stemmed.df_x, tfidf.compute_tfidf(), pp_stemmed.df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>how</th>\n",
       "      <th>unhappi</th>\n",
       "      <th>some</th>\n",
       "      <th>dog</th>\n",
       "      <th>like</th>\n",
       "      <th>it</th>\n",
       "      <th>though</th>\n",
       "      <th>talk</th>\n",
       "      <th>to</th>\n",
       "      <th>my</th>\n",
       "      <th>...</th>\n",
       "      <th>idfc</th>\n",
       "      <th>vikram</th>\n",
       "      <th>limay</th>\n",
       "      <th>diana</th>\n",
       "      <th>edulji</th>\n",
       "      <th>cag</th>\n",
       "      <th>4member</th>\n",
       "      <th>amulya</th>\n",
       "      <th>agmut</th>\n",
       "      <th>cadr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3821 rows × 5339 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      how  unhappi  some  dog  like  it  though  talk  to  my  ...  idfc  \\\n",
       "0       1        1     1    1     1   1       1     0   0   0  ...     0   \n",
       "1       0        0     0    0     0   1       0     1   3   1  ...     0   \n",
       "2       0        1     1    0     1   1       0     0   2   0  ...     0   \n",
       "3       0        1     0    0     0   0       0     0   1   0  ...     0   \n",
       "4       0        1     0    0     0   0       0     0   0   0  ...     0   \n",
       "...   ...      ...   ...  ...   ...  ..     ...   ...  ..  ..  ...   ...   \n",
       "3868    0        0     0    0     0   0       0     0   0   0  ...     1   \n",
       "3869    0        0     0    0     0   0       0     0   1   0  ...     0   \n",
       "3870    0        0     0    0     0   0       0     0   1   0  ...     0   \n",
       "3871    0        0     0    0     0   0       0     0   0   0  ...     0   \n",
       "3872    0        0     0    0     0   0       0     0   0   0  ...     0   \n",
       "\n",
       "      vikram  limay  diana  edulji  cag  4member  amulya  agmut  cadr  \n",
       "0          0      0      0       0    0        0       0      0     0  \n",
       "1          0      0      0       0    0        0       0      0     0  \n",
       "2          0      0      0       0    0        0       0      0     0  \n",
       "3          0      0      0       0    0        0       0      0     0  \n",
       "4          0      0      0       0    0        0       0      0     0  \n",
       "...      ...    ...    ...     ...  ...      ...     ...    ...   ...  \n",
       "3868       1      1      0       0    0        0       0      0     0  \n",
       "3869       0      0      1       1    0        0       0      0     0  \n",
       "3870       0      0      0       0    1        1       0      0     0  \n",
       "3871       0      0      0       0    0        0       0      0     0  \n",
       "3872       0      0      0       0    0        0       1      1     1  \n",
       "\n",
       "[3821 rows x 5339 columns]"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approaches['stemming'].counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lemmatizer(Processor):\n",
    "    def __init__(self, next_pileline, records=[]):\n",
    "        self.records = records\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        super().__init__(next_pileline)\n",
    "\n",
    "        \n",
    "    def process(self):\n",
    "        texts = []\n",
    "        for text in self.records:\n",
    "            words = []\n",
    "            for w in Extractor.extract_words(text):\n",
    "                words.append(self.lemmatizer.lemmatize(w, pos=Lemmatizer.get_wordnet_pos(w)))\n",
    "            texts.append(' '.join(words))\n",
    "        return texts\n",
    "\n",
    "    @classmethod\n",
    "    def get_wordnet_pos(cls, word):\n",
    "        pos = nltk.pos_tag([word])\n",
    "        treebank_tag = pos[0][1]\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return wordnet.NOUN\n",
    "\n",
    "\n",
    "all_lemmed = Tokenizer(Lemmatizer(None, records = negative_tweets), 'negative').begin_processing() \\\n",
    "+ Tokenizer(Lemmatizer(None, records = positive_tweets), 'positive').begin_processing() \\\n",
    "+ Tokenizer(Lemmatizer(None, records = neutral_tweets), 'neutral').begin_processing() \n",
    "\n",
    "pp_lemmed = PreProcessor(pd.DataFrame(all_lemmed))\n",
    "pp_lemmed.execute()\n",
    "\n",
    "tfidf_lemmed = TFIDFProcessor(pp_lemmed.df_x)\n",
    "\n",
    "approaches['lemmatization'] = Approach(binarize(pp_lemmed.df_x), pp_lemmed.df_x, tfidf_lemmed.compute_tfidf(), pp_lemmed.df_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misspelling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MisspellingsCorrector(Processor):\n",
    "    \n",
    "    def init_symspell(self):\n",
    "        sym_spell = SymSpell(max_dictionary_edit_distance=1)\n",
    "        dictionary_path = pkg_resources.resource_filename(\n",
    "            \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "        sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "        self.sym_spell = sym_spell\n",
    "        \n",
    "    def __init__(self, next_pipeline, records=[]):\n",
    "        self.records = records\n",
    "        self.init_symspell()\n",
    "        super().__init__(next_pipeline)\n",
    "    \n",
    "    def process(self):\n",
    "        texts = []\n",
    "        for text in self.records:\n",
    "            words = []\n",
    "            for w in Extractor.extract_words(text):\n",
    "                words.append(self.correct_word(w))\n",
    "            texts.append(' '.join(words))\n",
    "        return texts\n",
    "    \n",
    "    def correct_word(self, word: str) -> str:\n",
    "        corrections = self.sym_spell.lookup(word, Verbosity.CLOSEST, ignore_token=r\"\\w+\\d\")\n",
    "        if len(corrections) == 0:\n",
    "            return ''\n",
    "        else:\n",
    "            return corrections[0].term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming + misspellings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_stemmed_corrected = \\\n",
    "Tokenizer(Stemmer(MisspellingsCorrector(None, records=negative_tweets)), 'negative').begin_processing() \\\n",
    "+ Tokenizer(Stemmer(MisspellingsCorrector(None, records=positive_tweets)), 'positive').begin_processing() \\\n",
    "+ Tokenizer(Stemmer(MisspellingsCorrector(None, records=neutral_tweets)), 'neutral').begin_processing() \n",
    "\n",
    "pp_stemmed_corrected = PreProcessor(pd.DataFrame(all_stemmed_corrected))\n",
    "pp_stemmed_corrected.execute()\n",
    "\n",
    "tfidf_stemmed_corrected = TFIDFProcessor(pp_stemmed_corrected.df_x)\n",
    "\n",
    "approaches['s+m'] = Approach(binarize(pp_stemmed_corrected.df_x), pp_stemmed_corrected.df_x, tfidf_stemmed_corrected.compute_tfidf(), pp_stemmed_corrected.df_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization + misspellings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lemmed_corrected = Tokenizer(Lemmatizer(MisspellingsCorrector(None, records=negative_tweets)), 'negative').begin_processing() \\\n",
    "+ Tokenizer(Lemmatizer(MisspellingsCorrector(None, records=positive_tweets)), 'positive').begin_processing() \\\n",
    "+ Tokenizer(Lemmatizer(MisspellingsCorrector(None, records=neutral_tweets)), 'neutral').begin_processing() \n",
    "\n",
    "pp_lemmed_corrected = PreProcessor(pd.DataFrame(all_lemmed_corrected))\n",
    "pp_lemmed_corrected.execute()\n",
    "\n",
    "tfidf_lemmed_corrected = TFIDFProcessor(pp_lemmed_corrected.df_x)\n",
    "\n",
    "approaches['l+m'] = Approach(binarize(pp_lemmed_corrected.df_x), pp_lemmed_corrected.df_x, tfidf_lemmed_corrected.compute_tfidf(), pp_lemmed_corrected.df_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>how</th>\n",
       "      <th>unhappy</th>\n",
       "      <th>some</th>\n",
       "      <th>dog</th>\n",
       "      <th>like</th>\n",
       "      <th>it</th>\n",
       "      <th>though</th>\n",
       "      <th>talk</th>\n",
       "      <th>to</th>\n",
       "      <th>my</th>\n",
       "      <th>...</th>\n",
       "      <th>dept</th>\n",
       "      <th>hoarder</th>\n",
       "      <th>attache</th>\n",
       "      <th>payment</th>\n",
       "      <th>rs25000</th>\n",
       "      <th>historian</th>\n",
       "      <th>diana</th>\n",
       "      <th>appoint</th>\n",
       "      <th>gamut</th>\n",
       "      <th>cadre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>218</td>\n",
       "      <td>102</td>\n",
       "      <td>275</td>\n",
       "      <td>361</td>\n",
       "      <td>223</td>\n",
       "      <td>157</td>\n",
       "      <td>346</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>91</td>\n",
       "      <td>54</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3868</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3869</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>396</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3872</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3713 rows × 4417 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      how  unhappy  some  dog  like   it  though  talk  to  my  ...  dept  \\\n",
       "0     218      102   275  361   223  157     346     0   0   0  ...     0   \n",
       "1       0        0     0    0     0   48       0    87  91  54  ...     0   \n",
       "2       0       25    68    0     0   39       0     0  50   0  ...     0   \n",
       "3       0       90     0    0     0    0       0     0  87   0  ...     0   \n",
       "4       0      120     0    0     0    0       0     0   0   0  ...     0   \n",
       "...   ...      ...   ...  ...   ...  ...     ...   ...  ..  ..  ...   ...   \n",
       "3868    0        0     0    0     0    0       0     0   0   0  ...     0   \n",
       "3869    0        0     0    0     0    0       0     0  78   0  ...     0   \n",
       "3870    0        0     0    0     0    0       0     0  50   0  ...     0   \n",
       "3871    0        0     0    0     0    0       0     0   0  95  ...     0   \n",
       "3872    0        0     0    0     0    0       0     0   0   0  ...     0   \n",
       "\n",
       "      hoarder  attache  payment  rs25000  historian  diana  appoint  gamut  \\\n",
       "0           0        0        0        0          0      0        0      0   \n",
       "1           0        0        0        0          0      0        0      0   \n",
       "2           0        0        0        0          0      0        0      0   \n",
       "3           0        0        0        0          0      0        0      0   \n",
       "4           0        0        0        0          0      0        0      0   \n",
       "...       ...      ...      ...      ...        ...    ...      ...    ...   \n",
       "3868        0        0        0        0          0      0        0      0   \n",
       "3869        0        0        0        0          0    396        0      0   \n",
       "3870        0        0        0        0          0      0        0      0   \n",
       "3871        0        0        0        0          0      0        0      0   \n",
       "3872        0        0        0        0          0      0      274    274   \n",
       "\n",
       "      cadre  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "3         0  \n",
       "4         0  \n",
       "...     ...  \n",
       "3868      0  \n",
       "3869      0  \n",
       "3870      0  \n",
       "3871      0  \n",
       "3872    274  \n",
       "\n",
       "[3713 rows x 4417 columns]"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "approaches['l+m'].tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(approaches['tokenization'].tfidf, approaches['tokenization'].df_y['sentiment'], test_size=0.5)\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.867012987012987"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(lr.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5158858373721056"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(7)\n",
    "knn.fit(X_train, y_train)\n",
    "accuracy_score(knn.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4417"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lr.coef_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
